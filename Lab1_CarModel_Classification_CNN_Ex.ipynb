{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFoLOed-Vm5R"
   },
   "source": [
    "# Hand-on Lab 1 Ex : Car Model Classification with CNN\n",
    "In this handon Lab we are going to develop a Vehicle Classification using Computer Vision Machine Learning\n",
    "\n",
    "Python will the programming language used\n",
    "\n",
    "Tensorflow and Keras will the python modules use to develop the Machine Learning\n",
    "\n",
    "We will develop the followinf section of the codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   Install the python modules   \n",
    "2.   Add the libraries needed for the program\n",
    "3.  Prepare the data\n",
    "4.   Prepare the CNN model\n",
    "5.  Train the model with the training set and evaluate it performance\n",
    "6.   Use the trained model to classify input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gr9UQHf5JwE3"
   },
   "source": [
    "# 1. Install Python Modules\n",
    "Use the pip(python install program) to install the following modules\n",
    "* !pip install tensorflow==1.15.0\n",
    "* !pip install keras==2.1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hj1dHMgCLr9a"
   },
   "source": [
    "# 2. Add the libraries needed for the program\n",
    "From the installed modules add the libraries for the machine learning\n",
    "\n",
    "\n",
    "```\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2369,
     "status": "ok",
     "timestamp": 1600739644086,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "LjhErGYmV2Qi",
    "outputId": "129e7331-85da-4362-9c7d-4fc502d1be67"
   },
   "outputs": [],
   "source": [
    "#Add Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1600739647808,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "EAx4b2JXiWYm",
    "outputId": "e8e8bc65-31a7-46ae-836a-ff427ed175ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhr5JMTPMBlo"
   },
   "source": [
    "# 3. Prepare the data\n",
    "Following steps will prepare the data into training set and evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptJqGwgQNaDf"
   },
   "source": [
    "### 3.1 Create dir in drive and specific the path\n",
    "\n",
    "\n",
    "```\n",
    "In the path ./dataset/lab1dataset/data/\n",
    "create a train directory with following subdir\n",
    "train \n",
    "   |- Honda\n",
    "   |- Toyota\n",
    "   |- Volkswagen \n",
    "copy 60 different jpg imagea according to the category in each of the subdir\n",
    "\n",
    "train data path \n",
    "train_data_dir = data_dir_path+'train'\n",
    "```\n",
    "\n",
    "```\n",
    "In the path ./dataset/lab1dataset/data/\n",
    "create a train directory with following subdir\n",
    "test \n",
    "   |- Honda\n",
    "   |- Toyota\n",
    "   |- Volkswagen \n",
    "copy 60 different jpg imagea according to the category in each of the subdir\n",
    "\n",
    "Validation data path \n",
    "validation_data_dir = data_dir_path+'validation'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "In the path ./dataset/lab1dataset/\n",
    "creat a dir prediction_images\n",
    "Copy some test images inside this directory\n",
    "prediction_data_dir = './dataset/Lab1dataset/prediction_images/'\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1208,
     "status": "ok",
     "timestamp": 1600739785215,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "UqJN2hz7ZUy_"
   },
   "outputs": [],
   "source": [
    "# Define Paths \n",
    "#Add code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6KWkyz_PjU3"
   },
   "source": [
    "### 3.2 Get the total number of predicted classes\n",
    "From the train subdir we can get the number of classes\n",
    "\n",
    "\n",
    "> The numbers of classes are categories of object we wanted to predict\n",
    "\n",
    "> The numbers of subdir will be the classes name\n",
    "\n",
    "```\n",
    "classes = ImageDataGenerator().flow_from_directory(train_data_dir).class_indices\n",
    "print(classes)\n",
    "print(len(classes))\n",
    "num_classes= len(classes)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4137,
     "status": "ok",
     "timestamp": 1600739818578,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "e_REo5X_bhxa",
    "outputId": "7834c36b-22e6-4164-c6d2-a2ccf99327f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 images belonging to 3 classes.\n",
      "{'Honda': 0, 'Toyota': 1, 'Volkswagen': 2}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Gets the total no. of classes\n",
    "#Add code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDuVmIyrSoFe"
   },
   "source": [
    "### 3.3 Prepare Data for training and validation\n",
    "\n",
    "> Define the image width and height to be used as input to the model for training\n",
    "\n",
    ">Define the batch size \n",
    "\n",
    "```\n",
    "\n",
    "def PrepareData(img_width,img_height,batch_size ):\n",
    "\n",
    "    # This augments the data. This is usefull when working with a small sample size\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    print(\"train generator\")\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"validation generator\")\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "    return train_generator,validation_generator\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 927,
     "status": "ok",
     "timestamp": 1600739968263,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "DKKq1Nu0biPQ"
   },
   "outputs": [],
   "source": [
    "#Prepare Data for training and validation\n",
    "\n",
    "def PrepareData(img_width,img_height,batch_size ):\n",
    "\n",
    "    # This augments the data. This is usefull when working with a small sample size\n",
    "    # Add code train_datagen \n",
    "\n",
    "    #Add code validation_datagen\n",
    "\n",
    "    print(\"train generator\")\n",
    "    #Add code train_generator\n",
    "    \n",
    "    print(\"validation generator\")\n",
    "    #Add code validation_generator\n",
    "    \n",
    "    return train_generator,validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QO0RcRNQSS0-"
   },
   "source": [
    "# 4. Prepare the CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3-_hKmxN20M"
   },
   "source": [
    "### 4.1 CNN Convolution Netural Network\n",
    "> Use the ResNet50 CNN model\n",
    "\n",
    "> Each typical CNN layer comprise of Conv + Activation function + MaxPooling\n",
    "\n",
    "> Each CNN2D layer can be added into the Keras Squential \n",
    "\n",
    "> Define the parameters for each of the CNN layer\n",
    "\n",
    "*   Conv2D-> number of feature map , size of feature map size NxN\n",
    "*   Activation function -> relu , sigmo\n",
    "*   MaxPooling-> kernel size NxN\n",
    "\n",
    ">Define the ouput layer\n",
    "\n",
    "*   Flatten-> 2D to 1D\n",
    "*   Dense layer->number of neutrons\n",
    "*   Activation Function->softmax(probability of each the classes)\n",
    "\n",
    "```\n",
    "def compileModel(img_width, img_height,learning_rate=1e-4):\n",
    "    print(\"compiling model\")\n",
    "\n",
    "    # Insureing that the images are in the correct format.\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "    \n",
    "    img_color=3\n",
    "\n",
    "    conv_base = ResNet50(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(img_width, img_height, img_color))\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(conv_base)\n",
    "    #Add dense and classification layer\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # print(model.summary())\n",
    "    # print(conv_base.summary())\n",
    "    model.compile(loss='categorical_crossentropy',optimizer= optimizers.adam(lr=learning_rate),metrics=['accuracy'])\n",
    "    for layer in conv_base.layers:\n",
    "      layer.trainable = False\n",
    "    for layer in conv_base.layers[-4:]:\n",
    "      layer.trainable = True\n",
    "    return model\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1600740460029,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "KH0ec4qCf7FS"
   },
   "outputs": [],
   "source": [
    "def compileModel(img_width, img_height,learning_rate=1e-4):\n",
    "    print(\"compiling model\")\n",
    "\n",
    "    # Insureing that the images are in the correct format.\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "    \n",
    "    img_color=3\n",
    "\n",
    "    #Add code use the ResNet50 model\n",
    "    \n",
    "\n",
    "    # print(model.summary())\n",
    "    # print(conv_base.summary())\n",
    "    model.compile(loss='categorical_crossentropy',optimizer= optimizers.adam(lr=learning_rate),metrics=['accuracy'])\n",
    "    for layer in conv_base.layers:\n",
    "      layer.trainable = False\n",
    "    for layer in conv_base.layers[-4:]:\n",
    "      layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1600740131384,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "zMUegDZ9gY62"
   },
   "outputs": [],
   "source": [
    "best_model = keras.callbacks.ModelCheckpoint(data_dir_path+'custom_w_supervision_try2_best' + '.h5', monitor='val_acc',save_best_only=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss',factor=0.25, patience=5,min_lr=0.000005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAFGrHhTTIWX"
   },
   "source": [
    "# 5. Train the model with the training set and evaluate it performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1V4dlZyHUVTM"
   },
   "source": [
    "### 5.1 Setup Model with training and testing dataset\n",
    "\n",
    "> Define the train data generator/test data generator\n",
    "\n",
    "> Fit the train data generator into the CNN model for training\n",
    "\n",
    "> Start traning epochs\n",
    "\n",
    "> After training plot the performance metrics\n",
    "\n",
    "\n",
    "```\n",
    "def trainModel(train_data, validation_data,model):\n",
    "\n",
    "    print(\"starting training.... \")\n",
    "    hist = model.fit_generator(\n",
    "        (train_data),\n",
    "        steps_per_epoch=nb_train_samples // batch_size, # The accumulated amount of steps\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_data,\n",
    "        nb_val_samples=nb_validation_samples,\n",
    "        callbacks=[best_model, reduce_lr]\n",
    "    )\n",
    "\n",
    "    plotVal_plotLoss(hist)\n",
    "    model.save_weights(data_dir_path+'custom_w_supervision_try2.h5') # Saving the compile weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1600740440814,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "Nf9oc01xgnos"
   },
   "outputs": [],
   "source": [
    "def trainModel(train_data, validation_data,model):\n",
    "\n",
    "    print(\"starting training.... \")\n",
    "    #Add code to train with the data\n",
    "    #hsit= model.fir_generator()\n",
    "    \n",
    "    #Add code to disply the training result\n",
    "    \n",
    "    model.save_weights(data_dir_path+'custom_w_supervision_try2.h5') # Saving the compile weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1600740443959,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "PxFBEdKJg36i"
   },
   "outputs": [],
   "source": [
    "# This function generates graphs of the loss and the accuracy of the model\n",
    "def plotVal_plotLoss (model) :\n",
    "\n",
    "    plt.plot(model.history['acc'])\n",
    "    plt.plot(model.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('loss_plot_4 (simulated vgg1)2')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('loss_plot_4 (simulated vgg1)2')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Start the training\n",
    "```\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 197, 197\n",
    "batch_size = 30 # The batch size represents the total amount of pictures that are included in each iteration.\n",
    "\n",
    "train_data, validation_data = PrepareData(img_width,img_height,batch_size)\n",
    "\n",
    "model=compileModel(img_width, img_height,learning_rate=1e-4)\n",
    "\n",
    "# Defining the total amount of samples in both the training and validation set\n",
    "nb_train_samples = 180\n",
    "nb_validation_samples = 45\n",
    "epochs = 1\n",
    "\n",
    "trainModel(train_data, validation_data, model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 159950,
     "status": "ok",
     "timestamp": 1600741556038,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "RXNggLb7g4MG",
    "outputId": "7e0f3b08-0ce6-4028-9067-4ddb8b04d6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train generator\n",
      "Found 193 images belonging to 3 classes.\n",
      "validation generator\n",
      "Found 46 images belonging to 3 classes.\n",
      "compiling model\n",
      "WARNING:tensorflow:From e:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From e:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "starting training.... \n",
      "WARNING:tensorflow:From e:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "e:\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=6, epochs=1, validation_data=<keras.pre..., callbacks=[<keras.ca..., validation_steps=45)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "e:\\python37\\lib\\site-packages\\keras\\engine\\training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6/6 [==============================] - 246s 41s/step - loss: 1.1659 - acc: 0.4484 - val_loss: 0.8895 - val_acc: 0.6315\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = #add\n",
    "\n",
    "# The batch size represents the total amount of pictures that are included in each iteration.\n",
    "batch_size = #add\n",
    "\n",
    "#Prepare data\n",
    "#Add code\n",
    "train_data, validation_data = #Add code\n",
    "\n",
    "#Load Model\n",
    "model=# Add code\n",
    "\n",
    "# Defining the total amount of samples in both the training and validation set\n",
    "nb_train_samples = 180\n",
    "nb_validation_samples = 45\n",
    "epochs = #Add\n",
    "\n",
    "#Start the training model\n",
    "#Add code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBdPXDexTSsX"
   },
   "source": [
    "# 6. Use the trained model to classify input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Setup a predict image function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1600740763311,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "o_hvMM66goEz"
   },
   "outputs": [],
   "source": [
    "# This function c\n",
    "def predictImg(path, model):\n",
    "    imagep = image.load_img(path, target_size=(197, 197))\n",
    "    x = image.img_to_array(imagep)\n",
    "    x = x / 255  # Insures that images are normalized, so it can be compared test on a model that also used normalized training and validation images\n",
    "    #x = preprocess_input(x)\n",
    "    x = np.expand_dims(x, axis=0) # flattens the image\n",
    "    prediction = model.predict(x) # Extract the prediction made by the model\n",
    "    print(path)\n",
    "    print(prediction)\n",
    "    findLabel(prediction, 0.2, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1600740765510,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "b3efmUz5gxsj"
   },
   "outputs": [],
   "source": [
    "def findLabel(test, threshold, path):\n",
    "    if (max(test[0]) < threshold):\n",
    "        print(\"no class could be defined for \" + path + \" with threshold 0.85\")\n",
    "    else:\n",
    "        m = max(test[0])\n",
    "        index = [i for i, j in enumerate(list(test[0])) if j == m]\n",
    "        labeler(index[0], path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1600740774631,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "0trRjR9pgx_j"
   },
   "outputs": [],
   "source": [
    "def labeler(inp, pathname):\n",
    "    label = list(classes.keys())[inp]\n",
    "    print(\"The image '\" + pathname + \"' belongs to class: \" + label) # Prints the prediction\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Predict the unknow image\n",
    "> Setup the CNN Model\n",
    "\n",
    "> Load the trained weight(.h5) into the CNN model\n",
    "\n",
    "> Process the unknow input image and pass into the input of the CNN model\n",
    "\n",
    "> From the output probability(highest probability) find the label(class)\n",
    "\n",
    "```\n",
    "img_width, img_height = 197, 197\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "model = compileModel(img_width, img_height)\n",
    "\n",
    "model.load_weights(data_dir_path+'custom_w_supervision_try2_best.h5')\n",
    "\n",
    "# predictImg(prediction_data_dir + '/ambulance1.jpg, model) #ambulance\n",
    "predictImg(prediction_data_dir + '/toyota1.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/toyota2.jpg', model) # \n",
    "\n",
    "\n",
    "predictImg(prediction_data_dir + '/honda1.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/honda2.jpg', model) # \n",
    "\n",
    "predictImg(prediction_data_dir + '/volkwagen1.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/volkwagen2.jpg', model) # \n",
    "\n",
    "# predictImg(prediction_data_dir + '/ambulance1.jpg, model) #\n",
    "predictImg(prediction_data_dir + '/toyota3.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/toyota4.jpg', model) # \n",
    "\n",
    "\n",
    "predictImg(prediction_data_dir + '/honda3.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/honda4.jpg', model) # \n",
    "\n",
    "predictImg(prediction_data_dir + '/volkwagen3.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/volkwagen4.jpg', model) # \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60167,
     "status": "ok",
     "timestamp": 1600742011234,
     "user": {
      "displayName": "NYP weech",
      "photoUrl": "",
      "userId": "03211871160483530494"
     },
     "user_tz": -480
    },
    "id": "LRPFxU8PhC-6",
    "outputId": "f5ad104c-cd36-4906-e187-0d6afcc49a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n",
      "./Lab1dataset/prediction_images//toyota1.jpg\n",
      "[[0.    0.976 0.023]]\n",
      "The image './Lab1dataset/prediction_images//toyota1.jpg' belongs to class: Toyota\n",
      "./Lab1dataset/prediction_images//toyota2.jpg\n",
      "[[0. 1. 0.]]\n",
      "The image './Lab1dataset/prediction_images//toyota2.jpg' belongs to class: Toyota\n",
      "./Lab1dataset/prediction_images//honda1.jpg\n",
      "[[0.869 0.071 0.06 ]]\n",
      "The image './Lab1dataset/prediction_images//honda1.jpg' belongs to class: Honda\n",
      "./Lab1dataset/prediction_images//honda2.jpg\n",
      "[[0.998 0.001 0.001]]\n",
      "The image './Lab1dataset/prediction_images//honda2.jpg' belongs to class: Honda\n",
      "./Lab1dataset/prediction_images//volkwagen1.jpg\n",
      "[[0.001 0.    0.998]]\n",
      "The image './Lab1dataset/prediction_images//volkwagen1.jpg' belongs to class: Volkswagen\n",
      "./Lab1dataset/prediction_images//volkwagen2.jpg\n",
      "[[0. 0. 1.]]\n",
      "The image './Lab1dataset/prediction_images//volkwagen2.jpg' belongs to class: Volkswagen\n",
      "./Lab1dataset/prediction_images//toyota3.jpg\n",
      "[[0. 1. 0.]]\n",
      "The image './Lab1dataset/prediction_images//toyota3.jpg' belongs to class: Toyota\n",
      "./Lab1dataset/prediction_images//toyota4.jpg\n",
      "[[0. 1. 0.]]\n",
      "The image './Lab1dataset/prediction_images//toyota4.jpg' belongs to class: Toyota\n",
      "./Lab1dataset/prediction_images//honda3.jpg\n",
      "[[0.109 0.889 0.002]]\n",
      "The image './Lab1dataset/prediction_images//honda3.jpg' belongs to class: Toyota\n",
      "./Lab1dataset/prediction_images//honda4.jpg\n",
      "[[0.937 0.06  0.003]]\n",
      "The image './Lab1dataset/prediction_images//honda4.jpg' belongs to class: Honda\n",
      "./Lab1dataset/prediction_images//volkwagen3.jpg\n",
      "[[0.002 0.551 0.447]]\n",
      "The image './Lab1dataset/prediction_images//volkwagen3.jpg' belongs to class: Toyota\n",
      "./Lab1dataset/prediction_images//volkwagen4.jpg\n",
      "[[0.001 0.    0.999]]\n",
      "The image './Lab1dataset/prediction_images//volkwagen4.jpg' belongs to class: Volkswagen\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height = 197, 197\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "#Setup CNN Model\n",
    "#Add code \n",
    "\n",
    "#Load weight into Model \n",
    "#Add code\n",
    "\n",
    "\n",
    "\n",
    "# predictImg(prediction_data_dir + '/ambulance1.jpg, model) #ambulance\n",
    "predictImg(prediction_data_dir + '/toyota1.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/toyota2.jpg', model) # \n",
    "\n",
    "\n",
    "predictImg(prediction_data_dir + '/honda1.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/honda2.jpg', model) # \n",
    "\n",
    "predictImg(prediction_data_dir + '/volkwagen1.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/volkwagen2.jpg', model) # \n",
    "\n",
    "# predictImg(prediction_data_dir + '/ambulance1.jpg, model) #\n",
    "predictImg(prediction_data_dir + '/toyota3.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/toyota4.jpg', model) # \n",
    "\n",
    "\n",
    "predictImg(prediction_data_dir + '/honda3.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/honda4.jpg', model) # \n",
    "\n",
    "predictImg(prediction_data_dir + '/volkwagen3.jpg', model) # \n",
    "predictImg(prediction_data_dir + '/volkwagen4.jpg', model) # \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkruaBc4pLdY0qKoXWG6qt",
   "name": "Car3_Classification_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
